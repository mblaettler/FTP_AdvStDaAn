% !TeX spellcheck = en_GB
\section{Part1: Advanced regression modelling}

Depending on the response variable (the value we'd like to model) a different model must be choosen.

\begin{tabular}{|l|l|}
	\hline 
	\textbf{Response} & \textbf{Model} \\ 
	\hline 
	Continuous (with condtant variance) & Multiple linear regression model \\ 
	\hline 
	Zero/one variate & Logistic regression model \\ 
	\hline 
	Counts & Loglinear model \\ 
	\hline 
	Continuous (with constant coefficient of variation) & Gamma regression model \\ 
	\hline 
	Censored survival times & Accelerated Failure Times Models \\
	& Proportional hazard model \\
	\hline 
\end{tabular} 

\subsection{Linear Regression (Recap)}

Form: $y \approx \beta_0 + \beta_1 x_i^{(1)} + ... + \beta_m x_i^{(m)}$\\
With $x_i^{1...m}$ being the \textbf{predictor variables} and $y$ the \textbf{response variable}. If $m=1$ the method is called \textbf{simple regression} otherwise \textbf{multiple regression}. While the \textbf{predictor variables} $x_i^{1...m}$ can be a non-linear combination of variables, the $\beta$s can only be multiplied (hence \textbf{linear regression}).

\subsubsection{Example R-Code}
\begin{lstlisting}
library(datasets) # the dataset mtcars is located in this package
str(mtcars) # Compactly displays the internal structure of the object
summary(mtcars) # returns a six number summary of each variable in the data frame

mtcars1 <- data.frame(lMPG=log(mtcars$mpg), wCyl=sqrt(mtcars$cyl),
					  lDisp=log(mtcars$disp), lHP=log(mtcars$hp),
					  drat=mtcars$drat, lWT=log(mtcars$wt),
					  qsec=mtcars$qsec, VS=mtcars$vs, AM=mtcars$am,
					  wGear=sqrt(mtcars$gear), wCarb=sqrt(mtcars$carb))

mtc1.lm1 <- lm(lMPG ~ lDisp + lHP + lWT + drat + qsec + wCarb + wCyl
               + wGear + VS + AM, data=mtcars1)
\end{lstlisting}

Inspecting the \lstinline{summary(mtc1.lm1)} output:
\begin{lstlisting}
Call:
lm(formula = lMPG ~ lDisp + lHP + lWT + drat + qsec + wCarb + 
wCyl + wGear + VS + AM, data = mtcars1)

Residuals:
Min       1Q   Median       3Q      Max 
-0.16637 -0.06968  0.01446  0.05770  0.20108 

Coefficients:
Estimate Std. Error t value Pr(>|t|)  
(Intercept)  3.733511   1.325541   2.817   0.0103 *
lDisp       -0.167383   0.179541  -0.932   0.3618  
lHP         -0.159416   0.134980  -1.181   0.2508  
lWT         -0.377992   0.250297  -1.510   0.1459  
drat         0.004172   0.074864   0.056   0.9561  
qsec         0.014235   0.033289   0.428   0.6733  
wCarb       -0.149562   0.115652  -1.293   0.2100  
wCyl         0.188301   0.217927   0.864   0.3973  
wGear        0.449734   0.264221   1.702   0.1035  
VS          -0.040222   0.088789  -0.453   0.6552  
AM          -0.054940   0.098564  -0.557   0.5831  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1134 on 21 degrees of freedom
Multiple R-squared:  0.9018,	Adjusted R-squared:  0.8551 
F-statistic: 19.29 on 10 and 21 DF,  p-value: 2.097e-08
\end{lstlisting}

According to the F test for significance of regression, at least one of the explanatory variables is significant because its P-value of 3.649e-07 is smaller than 0.05. However, according to the marginal test none of the coefficients is significant on the 5\% level (except the intercept). This happens if the explanatory variables itself are (highly) correlated. To find a better model, a criterion must be chosen to find the optimal model.

\subsubsection{Akaike's information criterion}
A criterion to find better models:
\begin{equation*}
AIC = -2(maximized \: Log-Likelihood) + 2 \cdot \#estimates\: parameters
\end{equation*}
\begin{equation*}
AIC = n\cdot log(\frac{1}{n}\sum_{i=1}^{n} R_i^2) + 2p + constant
\end{equation*}
-> The criterion needs to be minimized

\paragraph{Use the R function \lstinline{step} to auto apply AIC}
\mbox{}
\begin{lstlisting}
mtc1.vs <- step(mtc1.lm1,
                scope=list(lower=~ 1,
                           upper=~ lDisp + lHP + drat + lWT + qsec
                                 + wCarb + fCyl + fVS + fAM + fGear))
\end{lstlisting}

While the variable \lstinline{mtc1.vs} now contains the model with the lowest AIC, a summary of the optimization process can be collected using \lstinline{mtc1.vs$anova}.

\paragraph{Compare two models using \lstinline{anova}}
ToDo %TODO

\subsubsection{Local Regression (LOESS)}
In comparison to linear model, where a \textbf{line} is fitted on the data, the LOESS regression can be used to fit a \textbf{curve}.

The concept of a LOESS regression is the following:

\begin{enumerate}
	\tightlist
	% See: https://www.youtube.com/watch?v=Vf7oJ6z2LCc
	\item For each point
	\begin{enumerate}
		\tightlist
		\item Set current point as \textbf{focal point}.
		\item Fit a straight line (\lstinline{family="gaussian"}) or a parabola (\lstinline{family="symetric"}) for the window. Inside the window, the distance of the point to the focal point determines the weight of each point.
		\item Set point at intercept with fitted line (\textbf{pre-fitted point})
	\end{enumerate}
	\item Calculate the difference for each point to the \textbf{pre-fitted point}
	\item For each pre-fitted point
	\begin{enumerate}
		\item Fit a line again using, but this time using the wights based on the distance to the \textbf{pre-fitted point}.
	\end{enumerate}
\end{enumerate}

\subsubsection{Residuals}

While the error term $E_i$ is unobservable it can be estimated by analysing the residuals:

\begin{equation*}
\vec{R} = \vec{Y} - \bm{X}\vec{\hat{\beta}} = \vec{Y} - \bm{H}\vec{Y} = (\bm{I}-\bm{H})\vec{Y}
\end{equation*}
\begin{equation*}
\bm{H} = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T
\end{equation*}

\paragraph{Scaled residuals}
\begin{equation*}
\breve{R_i} = \frac{R_i}{\sqrt{1-H_{ii}}}, i=1,...,n
\end{equation*}

\paragraph{Standardised residuals}
\begin{equation*}
\tilde{R_i} = \frac{R_i}{\sqrt{\hat{\sigma}^2(1-H_{ii}})}, i=1,...,n
\end{equation*}

\subsubsection{Diagnostic plots}
\begin{description}
	\tightlist
	\item[Tukey-Anscombe plot] Also called residual plot. Residuals against the fitted values: Is used to detect nonconstant expectation of the errors.
	\item[scale-lcation plot] The square-root of the absolute values of the standardized residuals against the fitted values: Is used to detect nonconstant variance of the errors.
	\item[normal Q-Q plot] Ordered standardized residuals against their expected values under normality: Is used to detect non normality of the errors.
	\item[Residuals against time and/or space variables] Is used to detect stochastic dependency in the errors.
	\item[sensitivity plot] Standardized residuals against leverage overlaid by Cook’s distance contours: Is used to detect too influential observation in the data.
\end{description}

\paragraph{Cook's distance}

\begin{equation*}
d_i = \frac{1}{p}\tilde{R_i}^2\frac{H_{ii}}{1-H_{ii}}
\end{equation*}

\textbf{Guideline:} Practical experience has shown that observations with a Cook’s distance $d_i$ larger than 1 can be considered as too influential.

\paragraph{Boostrap simulations}
Is an informal way to check stochastic fluctuation:
\begin{enumerate}
	\tightlist
	\item Generate observations $y_i^*$ according to the model using new random errors.\\
	$y_i^*=\hat{y_i}+\hat{\sigma}\cdot e_i^*$.
	\item Calculate the regression fit with the explanatory variables given in the data set and the newly generated response values $y_i^*$. Add the new lines to the plots.
	\item Repeat n times (n=19 for 5\% significance level).
\end{enumerate}

\subsubsection{Multicollinearity}
Is the term if some predictors are a linear combination of others.\\
Detect collinearity:
\begin{itemize}
	\tightlist
	\item Examination of the correlation matrix of the predictors will reveal large pairwise collinearities.
	\item The Variance Inflation Factor (VIF).\\
	\begin{lstlisting}
mtc3.lm0 <- lm(mpg ~ ., data=mtcars3)
vif(mtc3.lm0)
	\end{lstlisting}
	\textbf{Guideline:} If VIF is larger than 5 to 10 for any variable then multicollinearity is of a dangerous size and needs to be addressed.
\end{itemize}

\subsubsection{Categorical predictors}
If a predictor variable is categorical (e.g. a tool type) and has two levels, an indicator variable is set up:

\begin{equation*}
x_i = \begin{cases}
0 & if \; tool\; type\; A \\
1 & if\; tool\; type\; B
\end{cases}
\end{equation*}

In case of more than two levels, multiple indicator variables must be introduced:

\begin{tabular}{lll}
	$x_i^{2,2}$ & $x_i^{2,3}$  &  \\ 
	0 & 0 & for observation of type A \\ 
	1 & 0 & for observation of type B \\ 
	0 & 1 & for observation of type C \\ 
\end{tabular}

\subsubsection{Prediction error sum of squares (PRESS)}
In predictive modelling, the model are assessed often by the leave-one-out method. In this procedure, one observation is selected and the regression model is fitted to the remaining n-1 observations. With this we find the \textbf{prediction error for observation i}.
\begin{equation*}
r_{(i)}=y_i-\hat{y_(i)}
\end{equation*}
The \textbf{PRESS} is then defined by the sum of squares of the n prediction errors:
\begin{equation*}
PRSS = \sum_{i=1}^{n}r^2_{(i)} = \sum_{i=1}^{n}(y_i-\hat{y_{(i)}})^2
\end{equation*}

\subsubsection{Cross validation}
Cross validation is a model evaluation technique that tells how well the results will generalise to an independent dataset from the same population.\\
Basic idea (as example a 5-fold cross validation):
\begin{enumerate}
	\tightlist
	\item Split the data into n folds (for example chunks of 20\%)
	\item Use one fold for testing and the other for training
	\item Repeat step 2 n times until every fold is once used for testing
	\item Compute mean squared prediction error (MSPE)
	\begin{equation*}
		MSPE_{CV}=\frac{1}{n}\sum_{i=1}^{n}(\hat{y}_i^{Test} - y_i)^2
	\end{equation*}
\end{enumerate}

\subsubsection{Model building process}

\begin{enumerate}
	\tightlist
	\item Clarify the task (purpose? goal?); are there already model approaches?
	\item Data Screening and Processing
	\item Model fitting; preferably with robust methods
	\item Residual and sensitivity analysis; does this dataset help solve the task? -> Go back to 3, 2, 1 depending on the result
	\item Variable selection; treat collinearities if necessary -> Go back to 3 depending on the result
	\item Checking model adequacy
	\begin{itemize}
		\tightlist
		\item Residual and sensitivity analysis with selected model(s)
		\item Check plausibility; match model(s) with subject matter expertise
		\item Consider out-of-sample validation (i.e., validation with unused data), especially if the model is to be used for prediction.
	\end{itemize}
	-> Go back to 1, 2, 3, ... depending on the result
	\item Reporting: It is key to be honest and openly report all data manipulations and decisions that were made.
\end{enumerate}

\subsection{Advanced topics in linear regression modelling}

\subsubsection{Weighted least squares}

For the least squares regression to work, it is required to have an Error $\vec{E} \sim \mathcal{N}(\vec{0}, \sigma^2\bm{I})$ with $\bm{I}$ being the $n\times n$ identity matrix. In case of a \textbf{non-constant variance} called \textbf{heteroscedasticity} we need \textbf{weighted least squares} fitting. Therefore $\bm{I}$ is replaces with
\begin{equation*}
\bm{W}=diag[\frac{1}{w_1},\frac{1}{w_2}, ..., \frac{1}{w_n}] \neq \bm{I}
\end{equation*}

This allows us to give a weight for the samples we are more certain for the maximum likelihood calculation:

\begin{equation*}
\bm{X}^T\bm{W}\vec{R} = \vec{0}\;\; or \;\;
\bm{X}^T\bm{W}(\vec{Y}-\bm{X}\hat{\vec{\beta}}) = 0
\end{equation*}

\subsubsection{Robust fitting}
While real data is never exactly Gaussion distributed we need to find a model which is useful in many situations - hence for the majority of observations. The ordinary least squares (OLS) estimator cannot cope with this situation. A \textbf{robust} estimator is required.

\paragraph{The oldest, informal approach}

An easy common practise is to manually examine the data for obvious outliers and the manual removal of those. But this approach is problematic:
\begin{itemize}
	\tightlist
	\item Difficult or even impossible to identify outliers
	\item It is impossible to examine the relationship within the data without having a model in the first place
	\item The process of finding outliers is hard to formalize and therefore hard to automatize
	\item Inference based on applying a standard procedure to the cleaned data will be based on distribution theory which ignores the cleaning process and hence will be inapplicable and possibly misleading
\end{itemize}

\paragraph{Building a robust fitter}
By introducing weighted samples during the fitting process, samples with high residuals get applied a smaller wieght and therefore their influence is reduced.

\paragraph{M-estimator}
While the maximum likelihood estimator (OLS) tries to minimize the absolute residuals, the estimators belonging to the classes of M-estimators define a function $\rho$ which transforms the residuals (e.g. by clipping). Resulting in the following optimization function:
\begin{equation*}
\sum_{i=1}^{n}\psi(\tilde{r_i}) \cdot \vec{x_i} = \vec{0}
\end{equation*}
\begin{equation*}
\tilde{r_i} = \frac{Y_i-\vec{x_i}^T\vec{\hat{\beta_M}}}{\sigma}
\end{equation*}

The scale parameter $\sigma$ needs to be estimated. One possibility is to use the standard deviation, however this is extremely non-robust. It has been shown that a robust estimation is needed, called the MAV (=median of absolute value).
\begin{equation*}
	\sigma = s_{MAV} = \frac{median_i(|r_i|)}{0.6745}
\end{equation*}

\paragraph{MM-estimator}
\textbf{M}odified \textbf{M-estimator} improve the estimation of $\sigma$ since the M-estimator is not robust against leverage points. It uses the following procedure:
\begin{enumerate}
	\tightlist
	\item Find “good” initial values by optimising a slightly modified criterion (also called S-estimator)
	\item Apply a M-estimator but determine $\sigma$ from the estimation done in step 1.
\end{enumerate}

\subsubsection{Fitting smooth functions}
Another robust estimator is the \textbf{LOESS smoother} (\lstinline{loess.smooth}). It can either be fitted using an M-estimator (\lstinline{family="symmetric"}) oe by OLE (\lstinline{family="gaussian"}).

\paragraph{Cubic Spine Smoothing}
Is an alternative approach to local parametric regression for estimating a smooth function $f(\cdot)$ by a linear combination of some explicitly defined basis functions $h_m(\cdot)$. The number M controls the smoothness and the complexity of $f(\cdot)$.

\begin{equation*}
f(x) = \sum_{m=1}^{M}\gamma_m h_m(x)
\end{equation*}

This is highly subjected to overfitting for large M. Therefore the minimization function must penalized in case of high complexity. This can be achieved by introducing a function measuring the curvature.
\begin{equation*}
\sum_{i=1}^{n}(y_i-f(x_i))^2 + \lambda \int_{-\infty}^{\infty}f''(u) du
\end{equation*}

When analysing the above function it can be seen that $\lambda$ can also be fitted by doing a cross validation (e.g. leave-one-out) that it minimizes the expected squared prediction error.

\paragraph{Additive regression model and backfitting algorithm}
The cubic spine smoothing can be generalised to \textbf{(generalised) additive models}:
\begin{equation*}
Y_i = \beta_0 + f_1(x_i^{(1)}) + ... + f_m(x_i^{(m)}) + E_i
\end{equation*}

To estimate the flexible components for each predictor the backfitting algorithm is used:
\begin{enumerate}
	\tightlist
	\item Initialize $\hat{\beta_0} = \bar{y}$ and $\hat{f_k}(\cdot)=0$ for all $k=1, ..., m$ predictor variables
	\item Repeat for all $k=1, ..., m$ until convergence:
	\begin{enumerate}
		\tightlist
		\item Compute the partial residual $\tilde{y_i}^{(k)} = y_i - \hat{\beta_0}-\sum_{j=1, j\neq k}^{m} \hat{f_j}(x_i^{(j)})$
		\item Solve the 1-d smoothing problem using the data $(x_i^{(k)}, \tilde{y_i}^{(k)}) \rightarrow \hat{f_k}(\cdot)$
		\item Center the estimated function to $\hat{f_k}(\cdot) \leftarrow \hat{f_k}(\cdot) - \frac{1}{n} \sum_{i=1}^{n} \hat{f_k}(x_i^{(k)})$
	\end{enumerate}
\end{enumerate}

\subsection{Binary response}

\subsubsection{Sunflower-Plot and proportion plot}

The problem when plotting binary response variables is, that there are many measurements ``on the same point''. One way to overcome this problem when plotting binary response variable is to introduce a sunflower symbol for plotting. For every overlapping measurement, a new leaf is added to the sunflower.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{images/sunflower.png}
	\caption{Example sunflower plot}
\end{figure}

Another way of visualizing binary data is by calculating the proportion of ``ones'' for each corresponding input value $x$. The aggregated data $\tilde{Y_k}$ can then be plotted again using the sunflower plot.

\subsubsection{Logistic regression}

The problem when using linear regression is, that the prediction is not bounded. While the aggregated $\tilde{Y_k}$ is always within the interval $[0, 1]$ the predicted value can be outside of this range.

\mbox{}\\
Such a logistic regression must have the following properties:
\begin{enumerate}
	\tightlist
	\item The response Y is binary with possible values 0 and 1.\\
	In other words: $Y \sim Bernoulli(\pi=unknown)$
	\item $E(Y) = \pi$
	\item $\pi(x_k) = G(\beta_0 + \beta_1 \cdot x_k)$ or $G^{-1}(\pi(x_k)) = \beta_0 + \beta_1 \cdot x_k$\\
	With $x_k$ being the predictor variables and $G$ a distribution function.
	\item This results in a latent variable $Z =\eta(x_k) = \beta_0 + \beta_1 \cdot x_k$. The response $Y$ maps $\eta$ to 1 if $Z > c$ or vice versa.
	\item According to point 3: $\eta = G^{-1}(\pi)$. With $G^{-1}$ as \textbf{link function}.
\end{enumerate}

\paragraph{Popular choices for the distribution ($G()$) and link function ($G^{-1}()$)}\mbox{}\\
\begin{tabular}{ccl}
	\hline 
	$G(\eta)$ & $G^{-1}(\pi)$ & Name \\ 
	\hline 
	$\exp(\eta)/(1+\exp(\eta))$& $log(\pi/(1-\pi))$  & logit model; logistic regression (generally preferred) \\ 
	$\phi(\eta)$ & $\phi^{-1}(\eta)$ & probit model \\
	$1-exp(-exp(\eta))$ & $log(-log(1-\pi))$ & complementary log-log model \\
	\hline 
\end{tabular}

\paragraph{Odds}
Instead of focusing on the probability of occurrence of an event we turn our attention to the ‘odds’ for success versus failure:
\begin{equation*}
odds = \frac{P(Y_i=1)}{P(Y_i=0)} = \frac{P(Y_i=1)}{1 - P(Y_i=1)} = \frac{\pi}{1-\pi}
\end{equation*}

Since the range of odds is between 0 and infinity, the log-odds (logarithmic odds) are often preferred:
\begin{equation*}
log(odds) = log(\frac{\pi}{1-\pi}) = \eta = \beta_0 + \sum_{j=1}^{m} \beta_j x^{(j)}
\end{equation*}

\paragraph{Empirical logits}

Because the logit model is not defined for $G^{-1}(0)$ and $G^{-1}(1)$ (Latent variable $Z = g(Y)$) the empirical logits are often used:
\begin{equation*}
Z_k = log(\frac{Y_k + 0.5}{m_k-Y_k+0.5})
\end{equation*}

\subsubsection{Maximum Likelihood Estimator (MLE)}

While the previously introduced estimators (e.g. least squares estimator) is good for fitting continuous (Gaussian like) data, it is not suitable for binary response variables where the response is either 0 or 1. In comparison th MLE is better suited for fitting non Gaussian like data because it can work with any distribution since it fits the parameter of the distribution.

\subsection{Generalised linear model}

The components introduced for logistic regression can be summarized and generalized even further if the following properties are given:

\begin{itemize}
	\tightlist
	\item A set of explanatory variables ($x^{(1)}, x^{(2)}, ..., x^{(m)}$)
	\item A response variable $Y$ with expectation $E(Y)=\mu$
	\item A link function $G^{-1} = g$ such that $g(\mu) = \vec{x}^T \vec{\beta}$
	\item A distribution for the variability in $Y$
\end{itemize}

These conditions are met if the probability function $f$ of $Y$ can be written as
\begin{equation*}
f(y_i, \mu_i, \phi) = exp(\frac{y_i b(\mu_i)-c(\mu_i)}{\phi}w_i+d(y_i, \phi, w_i))
\end{equation*}
With:
\begin{description}
	\tightlist
	\item[$b()$ and $c()$] Determine the specific distribution
	\item[$d()$] Normalizes the function to a total of 1
	\item[$\phi$] Dispersion parameter
	\item[$w_i$] Known number but may vary from observation to observation.	It has the meaning of a weight.
\end{description}

This results in the following parameters for common distributions:

\begin{tabular}{l||rr|rrcc}
	\hline 
	Distribution & $E(Y) = \mu$ & $var(Y)$ & $b(\mu)$ & $V(\mu)$ & $\phi$ & w \\
	\hline
	$Gaussion(\mu, \sigma^2)$ & $\mu$ & $\sigma^2$ & $\mu$ & 1 & $\sigma^2$ & 1\\
	$Binomial(m, \pi)$ & $\pi$ & $\frac{1}{m}\pi(1-\pi)$ & $log(\frac{\mu}{1-\mu})$ & $\mu(1-\mu)$ & 1 & m\\
	$Poisson(\lambda)$ & $\lambda$ & $\lambda$ & $log(\mu)$ & $\mu$ & 1 & 1\\
	$Gamma(\alpha, \beta)$ & $\frac{\alpha}{\beta}$ & $\frac{\alpha}{\beta^2}$ & $-\frac{1}{\mu}$ & $\mu^2$ & $\frac{1}{\alpha}$ & 1\\
	\hline
\end{tabular}

\paragraph{Link function}
The corresponding inverse link function $G() = g^{-1}()$ should map the values of the linear predictor $\eta = \vec{x}^T \vec{\beta}$ to the support of the expected value of $Y$.\\
Obvious link functions:
\begin{description}
	\item[Identity] $g(\mu) = \mu$ if $Y$ is not subject to any restrictions on $\mathbb{R}$
	\item[Log] $g(\mu) = log(\mu)$ if $Y$ must be a positive real number
	\item[Logit] $g(\mu) = logit(\mu)=log(\mu/(1-\mu)$ if $Y$ must be between 0 and 1 (e.g. logistic regression)
\end{description}

\paragraph{Canonical link}
For every distribution a so called canonical link function (clf) exists which transforms $\eta$ to the defined distribution.

\paragraph{Support in R}
\lstinline{glm(..., family=poisson(link=identity))} a GLM can be fitted using different link functions. The default function is the canonical link function.

\begin{tabular}{c|cccccc}
	& binomial & gaussian & Gamma & inverse.gaussian & poisson & quasi \\
	\hline 
	logit & D & & & & & x \\
	probit & x & & & & & x \\
	cauchit & x & & & & &  \\
	clolog & x & & & & & x \\
	identity & & D & x & & x & x \\
	inverse & & & D & & & x \\
	log & x & & x & & D & x \\
	$1/\mu^2$ & & & & D & & x \\
	sqrt & & & & & x & x \\
\end{tabular}

\subsubsection{Fitting a GLM}

Like a logistic model, a GLM is also fitted using a maximum likelihood estimator (MLE). For a robust fitting the \textbf{IRLS (Iteratively reweighted least squares) algorithm} can be used. It is basically a weighted MLE estimator to lower the influence of outliers.
