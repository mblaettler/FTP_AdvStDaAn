% !TeX spellcheck = en_GB
\section{Part1: Advanced regression modelling}

Depending on the response variable (the value we'd like to model) a different model must be choosen.

\begin{tabular}{|l|l|}
	\hline 
	\textbf{Response} & \textbf{Model} \\ 
	\hline 
	Continuous (with condtant variance) & Multiple linear regression model \\ 
	\hline 
	Zero/one variate & Logistic regression model \\ 
	\hline 
	Counts & Loglinear model \\ 
	\hline 
	Continuous (with constant coefficient of variation) & Gamma regression model \\ 
	\hline 
	Censored survival times & Accelerated Failure Times Models \\
	& Proportional hazard model \\
	\hline 
\end{tabular} 

\subsection{Linear Regression (Recap)}

Form: $y \approx \beta_0 + \beta_1 x_i^{(1)} + ... + \beta_m x_i^{(m)}$\\
With $x_i^{1...m}$ being the \textbf{predictor variables} and $y$ the \textbf{response variable}. If $m=1$ the method is called \textbf{simple regression} otherwise \textbf{multiple regression}. While the \textbf{predictor variables} $x_i^{1...m}$ can be a non-linear combination of variables, the $\beta$s can only be multiplied (hence \textbf{linear regression}).

\subsubsection{Example R-Code}
\begin{lstlisting}
library(datasets) # the dataset mtcars is located in this package
str(mtcars) # Compactly displays the internal structure of the object
summary(mtcars) # returns a six number summary of each variable in the data frame

mtcars1 <- data.frame(lMPG=log(mtcars$mpg), wCyl=sqrt(mtcars$cyl),
					  lDisp=log(mtcars$disp), lHP=log(mtcars$hp),
					  drat=mtcars$drat, lWT=log(mtcars$wt),
					  qsec=mtcars$qsec, VS=mtcars$vs, AM=mtcars$am,
					  wGear=sqrt(mtcars$gear), wCarb=sqrt(mtcars$carb))

mtc1.lm1 <- lm(lMPG ~ lDisp + lHP + lWT + drat + qsec + wCarb + wCyl
               + wGear + VS + AM, data=mtcars1)
\end{lstlisting}

Inspecting the \lstinline{summary(mtc1.lm1)} output:
\begin{lstlisting}
Call:
lm(formula = lMPG ~ lDisp + lHP + lWT + drat + qsec + wCarb + 
wCyl + wGear + VS + AM, data = mtcars1)

Residuals:
Min       1Q   Median       3Q      Max 
-0.16637 -0.06968  0.01446  0.05770  0.20108 

Coefficients:
Estimate Std. Error t value Pr(>|t|)  
(Intercept)  3.733511   1.325541   2.817   0.0103 *
lDisp       -0.167383   0.179541  -0.932   0.3618  
lHP         -0.159416   0.134980  -1.181   0.2508  
lWT         -0.377992   0.250297  -1.510   0.1459  
drat         0.004172   0.074864   0.056   0.9561  
qsec         0.014235   0.033289   0.428   0.6733  
wCarb       -0.149562   0.115652  -1.293   0.2100  
wCyl         0.188301   0.217927   0.864   0.3973  
wGear        0.449734   0.264221   1.702   0.1035  
VS          -0.040222   0.088789  -0.453   0.6552  
AM          -0.054940   0.098564  -0.557   0.5831  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1134 on 21 degrees of freedom
Multiple R-squared:  0.9018,	Adjusted R-squared:  0.8551 
F-statistic: 19.29 on 10 and 21 DF,  p-value: 2.097e-08
\end{lstlisting}

According to the F test for significance of regression, at least one of the explanatory variables is significant because its P-value of 3.649e-07 is smaller than 0.05. However, according to the marginal test none of the coefficients is significant on the 5\% level (except the intercept). This happens if the explanatory variables itself are (highly) correlated. To find a better model, a criterion must be chosen to find the optimal model.

\subsubsection{Akaike's information criterion}
A criterion to find better models:
\begin{equation*}
AIC = -2(maximized \: Log-Likelihood) + 2 \cdot \#estimates\: parameters
\end{equation*}
\begin{equation*}
AIC = n\cdot log(\frac{1}{n}\sum_{i=1}^{n} R_i^2) + 2p + constant
\end{equation*}
-> The criterion needs to be minimized

\paragraph{Use the R function \lstinline{step} to auto apply AIC}
\mbox{}
\begin{lstlisting}
mtc1.vs <- step(mtc1.lm1,
                scope=list(lower=~ 1,
                           upper=~ lDisp + lHP + drat + lWT + qsec
                                 + wCarb + fCyl + fVS + fAM + fGear))
\end{lstlisting}

While the variable \lstinline{mtc1.vs} now contains the model with the lowest AIC, a summary of the optimization process can be collected using \lstinline{mtc1.vs$anova}.

\paragraph{Compare two models using \lstinline{anova}}
ToDo %TODO

\subsubsection{Local Regression (LOESS)}
In comparison to linear model, where a \textbf{line} is fitted on the data, the LOESS regression can be used to fit a \textbf{curve}.

The concept of a LOESS regression is the following:

\begin{enumerate}
	\tightlist
	% See: https://www.youtube.com/watch?v=Vf7oJ6z2LCc
	\item For each point
	\begin{enumerate}
		\tightlist
		\item Set current point as \textbf{focal point}.
		\item Fit a straight line (\lstinline{family="gaussian"}) or a parabola (\lstinline{family="symetric"}) for the window. Inside the window, the distance of the point to the focal point determines the weight of each point.
		\item Set point at intercept with fitted line (\textbf{pre-fitted point})
	\end{enumerate}
	\item Calculate the difference for each point to the \textbf{pre-fitted point}
	\item For each pre-fitted point
	\begin{enumerate}
		\item Fit a line again using, but this time using the wights based on the distance to the \textbf{pre-fitted point}.
	\end{enumerate}
\end{enumerate}

\subsubsection{Residuals}

While the error term $E_i$ is unobservable it can be estimated by analysing the residuals:

\begin{equation*}
\vec{R} = \vec{Y} - \bm{X}\vec{\hat{\beta}} = \vec{Y} - \bm{H}\vec{Y} = (\bm{I}-\bm{H})\vec{Y}
\end{equation*}
\begin{equation*}
\bm{H} = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T
\end{equation*}

\paragraph{Scaled residuals}
\begin{equation*}
\breve{R_i} = \frac{R_i}{\sqrt{1-H_{ii}}}, i=1,...,n
\end{equation*}

\paragraph{Standardised residuals}
\begin{equation*}
\tilde{R_i} = \frac{R_i}{\sqrt{\hat{\sigma}^2(1-H_{ii}})}, i=1,...,n
\end{equation*}

\subsubsection{Diagnostic plots}
\begin{description}
	\tightlist
	\item[Tukey-Anscombe plot] Also called residual plot. Residuals against the fitted values: Is used to detect nonconstant expectation of the errors.
	\item[scale-lcation plot] The square-root of the absolute values of the standardized residuals against the fitted values: Is used to detect nonconstant variance of the errors.
	\item[normal Q-Q plot] Ordered standardized residuals against their expected values under normality: Is used to detect non normality of the errors.
	\item[Residuals against time and/or space variables] Is used to detect stochastic dependency in the errors.
	\item[sensitivity plot] Standardized residuals against leverage overlaid by Cook’s distance contours: Is used to detect too influential observation in the data.
\end{description}

\paragraph{Cook's distance}

\begin{equation*}
d_i = \frac{1}{p}\tilde{R_i}^2\frac{H_{ii}}{1-H_{ii}}
\end{equation*}

\textbf{Guideline:} Practical experience has shown that observations with a Cook’s distance $d_i$ larger than 1 can be considered as too influential.

\paragraph{Boostrap simulations}
Is an informal way to check stochastic fluctuation:
\begin{enumerate}
	\tightlist
	\item Generate observations $y_i^*$ according to the model using new random errors.\\
	$y_i^*=\hat{y_i}+\hat{\sigma}\cdot e_i^*$.
	\item Calculate the regression fit with the explanatory variables given in the data set and the newly generated response values $y_i^*$. Add the new lines to the plots.
	\item Repeat n times (n=19 for 5\% significance level).
\end{enumerate}

\subsubsection{Multicollinearity}
Is the term if some predictors are a linear combination of others.\\
Detect collinearity:
\begin{itemize}
	\tightlist
	\item Examination of the correlation matrix of the predictors will reveal large pairwise collinearities.
	\item The Variance Inflation Factor (VIF).\\
	\begin{lstlisting}
mtc3.lm0 <- lm(mpg ~ ., data=mtcars3)
vif(mtc3.lm0)
	\end{lstlisting}
	\textbf{Guideline:} If VIF is larger than 5 to 10 for any variable then multicollinearity is of a dangerous size and needs to be addressed.
\end{itemize}

\subsubsection{Categorical predictors}
If a predictor variable is categorical (e.g. a tool type) and has two levels, an indicator variable is set up:

\begin{equation*}
x_i = \begin{cases}
0 & if \; tool\; type\; A \\
1 & if\; tool\; type\; B
\end{cases}
\end{equation*}

In case of more than two levels, multiple indicator variables must be introduced:

\begin{tabular}{lll}
	$x_i^{2,2}$ & $x_i^{2,3}$  &  \\ 
	0 & 0 & for observation of type A \\ 
	1 & 0 & for observation of type B \\ 
	0 & 1 & for observation of type C \\ 
\end{tabular}

\subsubsection{Prediction error sum of squares (PRESS)}
In predictive modelling, the model are assessed often by the leave-one-out method. In this procedure, one observation is selected and the regression model is fitted to the remaining n-1 observations. With this we find the \textbf{prediction error for observation i}.
\begin{equation*}
r_{(i)}=y_i-\hat{y_(i)}
\end{equation*}
The \textbf{PRESS} is then defined by the sum of squares of the n prediction errors:
\begin{equation*}
PRSS = \sum_{i=1}^{n}r^2_{(i)} = \sum_{i=1}^{n}(y_i-\hat{y_{(i)}})^2
\end{equation*}

\subsubsection{Cross validation}
Cross validation is a model evaluation technique that tells how well the results will generalise to an independent dataset from the same population.\\
Basic idea (as example a 5-fold cross validation):
\begin{enumerate}
	\tightlist
	\item Split the data into n folds (for example chunks of 20\%)
	\item Use one fold for testing and the other for training
	\item Repeat step 2 n times until every fold is once used for testing
	\item Compute mean squared prediction error (MSPE)
	\begin{equation*}
		MSPE_{CV}=\frac{1}{n}\sum_{i=1}^{n}(\hat{y}_i^{Test} - y_i)^2
	\end{equation*}
\end{enumerate}

\subsubsection{Model building process}

\begin{enumerate}
	\tightlist
	\item Clarify the task (purpose? goal?); are there already model approaches?
	\item Data Screening and Processing
	\item Model fitting; preferably with robust methods
	\item Residual and sensitivity analysis; does this dataset help solve the task? -> Go back to 3, 2, 1 depending on the result
	\item Variable selection; treat collinearities if necessary -> Go back to 3 depending on the result
	\item Checking model adequacy
	\begin{itemize}
		\tightlist
		\item Residual and sensitivity analysis with selected model(s)
		\item Check plausibility; match model(s) with subject matter expertise
		\item Consider out-of-sample validation (i.e., validation with unused data), especially if the model is to be used for prediction.
	\end{itemize}
	-> Go back to 1, 2, 3, ... depending on the result
	\item Reporting: It is key to be honest and openly report all data manipulations and decisions that were made.
\end{enumerate}

\subsection{Advanced topics in linear regression modelling}

\subsubsection{Weighted least squares}

For the least squares regression to work, it is required to have an Error $\vec{E} \sim \mathcal{N}(\vec{0}, \sigma^2\bm{I})$ with $\bm{I}$ being the $n\times n$ identity matrix. In case of a \textbf{non-constant variance} called \textbf{heteroscedasticity} we need \textbf{weighted least squares} fitting. Therefore $\bm{I}$ is replaces with
\begin{equation*}
\bm{W}=diag[\frac{1}{w_1},\frac{1}{w_2}, ..., \frac{1}{w_n}] \neq \bm{I}
\end{equation*}

This allows us to give a weight for the samples we are more certain for the maximum likelihood calculation:

\begin{equation*}
\bm{X}^T\bm{W}\vec{R} = \vec{0}\;\; or \;\;
\bm{X}^T\bm{W}(\vec{Y}-\bm{X}\hat{\vec{\beta}}) = 0
\end{equation*}

\subsubsection{Robust fitting}
While real data is never exactly Gaussion distributed we need to find a model which is useful in many situations - hence for the majority of observations. The ordinary least squares (OLS) estimator cannot cope with this situation. A \textbf{robust} estimator is required.

\paragraph{The oldest, informal approach}

An easy common practise is to manually examine the data for obvious outliers and the manual removal of those. But this approach is problematic:
\begin{itemize}
	\tightlist
	\item Difficult or even impossible to identify outliers
	\item It is impossible to examine the relationship within the data without having a model in the first place
	\item The process of finding outliers is hard to formalize and therefore hard to automatize
	\item Inference based on applying a standard procedure to the cleaned data will be based on distribution theory which ignores the cleaning process and hence will be inapplicable and possibly misleading
\end{itemize}

\paragraph{Building a robust fitter}
By introducing weighted samples during the fitting process, samples with high residuals get applied a smaller wieght and therefore their influence is reduced.

\paragraph{M-estimator}
While the maximum likelihood estimator (OLS) tries to minimize the absolute residuals, the estimators belonging to the classes of M-estimators define a function $\rho$ which transforms the residuals (e.g. by clipping). Resulting in the following optimization function:
\begin{equation*}
\sum_{i=1}^{n}\psi(\tilde{r_i}) \cdot \vec{x_i} = \vec{0}
\end{equation*}
\begin{equation*}
\tilde{r_i} = \frac{Y_i-\vec{x_i}^T\vec{\hat{\beta_M}}}{\sigma}
\end{equation*}

The scale parameter $\sigma$ needs to be estimated. One possibility is to use the standard deviation, however this is extremely non-robust. It has been shown that a robust estimation is needed, called the MAV (=median of absolute value).
\begin{equation*}
	\sigma = s_{MAV} = \frac{median_i(|r_i|)}{0.6745}
\end{equation*}

\paragraph{MM-estimator}
\textbf{M}odified \textbf{M-estimator} improve the estimation of $\sigma$ since the M-estimator is not robust against leverage points. It uses the following procedure:
\begin{enumerate}
	\tightlist
	\item Find “good” initial values by optimising a slightly modified criterion (also called S-estimator)
	\item Apply a M-estimator but determine $\sigma$ from the estimation done in step 1.
\end{enumerate}

\subsubsection{Fitting smooth functions}
Another robust estimator is the \textbf{LOESS smoother} (\lstinline{loess.smooth}). It can either be fitted using an M-estimator (\lstinline{family="symmetric"}) oe by OLE (\lstinline{family="gaussian"}).

\paragraph{Cubic Spine Smoothing}
Is an alternative approach to local parametric regression for estimating a smooth function $f(\cdot)$ by a linear combination of some explicitly defined basis functions $h_m(\cdot)$. The number M controls the smoothness and the complexity of $f(\cdot)$.

\begin{equation*}
f(x) = \sum_{m=1}^{M}\gamma_m h_m(x)
\end{equation*}

This is highly subjected to overfitting for large M. Therefore the minimization function must penalized in case of high complexity. This can be achieved by introducing a function measuring the curvature.
\begin{equation*}
\sum_{i=1}^{n}(y_i-f(x_i))^2 + \lambda \int_{-\infty}^{\infty}f''(u) du
\end{equation*}

When analysing the above function it can be seen that $\lambda$ can also be fitted by doing a cross validation (e.g. leave-one-out) that it minimizes the expected squared prediction error.

\paragraph{Additive regression model and backfitting algorithm}
The cubic spine smoothing can be generalised to \textbf{(generalised) additive models}:
\begin{equation*}
Y_i = \beta_0 + f_1(x_i^{(1)}) + ... + f_m(x_i^{(m)}) + E_i
\end{equation*}

To estimate the flexible components for each predictor the backfitting algorithm is used:
\begin{enumerate}
	\tightlist
	\item Initialize $\hat{\beta_0} = \bar{y}$ and $\hat{f_k}(\cdot)=0$ for all $k=1, ..., m$ predictor variables
	\item Repeat for all $k=1, ..., m$ until convergence:
	\begin{enumerate}
		\tightlist
		\item Compute the partial residual $\tilde{y_i}^{(k)} = y_i - \hat{\beta_0}-\sum_{j=1, j\neq k}^{m} \hat{f_j}(x_i^{(j)})$
		\item Solve the 1-d smoothing problem using the data $(x_i^{(k)}, \tilde{y_i}^{(k)}) \rightarrow \hat{f_k}(\cdot)$
		\item Center the estimated function to $\hat{f_k}(\cdot) \leftarrow \hat{f_k}(\cdot) - \frac{1}{n} \sum_{i=1}^{n} \hat{f_k}(x_i^{(k)})$
	\end{enumerate}
\end{enumerate}
